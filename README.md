# Distributed Algorithm Optimization
Perceptron algorithm is widely used in the natural language processing and the
machine learning. It intrinsically requires that the whole dataset is kept in one machine.
In the distributed systems, however, to improve the speed of the calculation, an important
strategy is dividing the whole dataset into several partitions across the clusters and
performing parallel calculations. Here, I proposed several models to solve this
contradiction. The results showed that my models decrease the number of messages
among the clusters and improve the performance. I implemented Perceptron algorithm
in the distributed systems with Apache Spark on Amazon Elastic Compute Cloud (EC2).
With this design, I have achieved the machine learning with security, high scalability,
high speed, and fault tolerance.

# Methods
<p></p>
![Alt text](https://github.com/Charley-Wang/Distributed-Algorithm-Optimization/blob/master/results/Model0.jpg?raw=true "Main Interface")
<p></p>
![Alt text](https://github.com/Charley-Wang/Distributed-Algorithm-Optimization/blob/master/results/Model1.jpg?raw=true "Main Interface")
<p></p>
![Alt text](https://github.com/Charley-Wang/Distributed-Algorithm-Optimization/blob/master/results/Model2.jpg?raw=true "Main Interface")
<p></p>
![Alt text](https://github.com/Charley-Wang/Distributed-Algorithm-Optimization/blob/master/results/Model3-6.jpg?raw=true "Main Interface")
